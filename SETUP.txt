================================================================
  TESTING ENGINE — SETUP & RUN INSTRUCTIONS
================================================================

WHAT THIS TOOL DOES:
  An AI-powered test failure analysis engine. It pulls commit
  history from a GitHub repo, loads your Spark HTML test reports,
  and uses Azure OpenAI (GPT-4) to analyze root causes of failures.

----------------------------------------------------------------
  PREREQUISITES
----------------------------------------------------------------

  - Python 3.9 or higher
      Download: https://www.python.org/downloads/
      Verify:   python --version

  - Git
      Download: https://git-scm.com/downloads
      Verify:   git --version

  - Azure OpenAI access (with a deployed GPT-4 model)
      You need: API Key, Endpoint URL, Deployment name, API version

  - A GitHub Personal Access Token (PAT)
      For pulling private repo data via git_extractor.py
      Scope needed: repo (read)

----------------------------------------------------------------
  STEP 1 — CLONE THE REPOSITORY
----------------------------------------------------------------

  git clone https://github.com/VICKYPATIL2525/test-engine.git
  cd test-engine

----------------------------------------------------------------
  STEP 2 — CREATE A VIRTUAL ENVIRONMENT
----------------------------------------------------------------

  Windows:
    python -m venv venv
    venv\Scripts\activate

  Mac / Linux:
    python3 -m venv venv
    source venv/bin/activate

  You should see (venv) at the start of your terminal prompt.

----------------------------------------------------------------
  STEP 3 — INSTALL DEPENDENCIES
----------------------------------------------------------------

  Full install (recommended — includes git extraction):
    pip install -r requirements.txt

  Packages installed:
    streamlit, openai, langchain-openai, langchain-core,
    gitpython, python-dotenv

  Minimal install (only run strapp.py / app.py, no git extraction):
    pip install -r requirements-minimal.txt

  Packages installed:
    streamlit, openai, langchain-openai, langchain-core,
    python-dotenv

----------------------------------------------------------------
  STEP 4 — CONFIGURE ENVIRONMENT VARIABLES
----------------------------------------------------------------

  Copy the example env file and fill in your credentials:

    Windows:   copy .env.example .env
    Mac/Linux: cp .env.example .env

  Then open .env in any text editor and fill in:

    # Azure OpenAI (required)
    OPENAI_API_KEY=<your Azure OpenAI API key>
    AZURE_OPENAI_ENDPOINT=https://<your-resource>.openai.azure.com/
    AZURE_OPENAI_VERSION=2024-12-01-preview
    AZURE_OPENAI_DEPLOYMENT=gpt-4.1-mini

    # GitHub (required for git_extractor.py)
    GITHUB_TOKEN=<your GitHub personal access token>
    REPO_URL=https://github.com/<org>/<repo>.git

    # Extraction settings (optional, defaults shown)
    MAX_COMMITS=50
    SHALLOW_CLONE=True
    TIME_RANGE_DAYS=30

  NOTE: Your team lead will share the actual credentials with you
  directly. Do NOT commit your .env file — it is already listed
  in .gitignore and will not be uploaded to GitHub.

----------------------------------------------------------------
  STEP 5 — PULL REPOSITORY DATA (git_extractor.py)
----------------------------------------------------------------

  This step extracts commit history from your target GitHub repo.

    python git_extractor.py

  Output is saved inside:  pulled_data/<repo-name>_<timestamp>/

  Files created:
    - commits_detailed.json   (full commit history)
    - repository_info.json    (repo metadata)
    - contributors.json       (contributor list)

  You can control extraction depth in .env:
    MAX_COMMITS=50
    SHALLOW_CLONE=True
    TIME_RANGE_DAYS=30

----------------------------------------------------------------
  STEP 6 — ADD YOUR HTML TEST REPORTS
----------------------------------------------------------------

  Create the html-reports folder in the project root (if it does
  not already exist) and place your Spark HTML report files in it:

    test-engine/
    └── html-reports/
        ├── run1_TestRunReport.html
        ├── run2_TestRunReport.html
        └── ...

  These are the HTML reports generated by your test framework
  (e.g. ExtentReports / Spark). The AI will read these to
  identify failures.

----------------------------------------------------------------
  STEP 7 — RUN THE STREAMLIT WEB APP (Recommended)
----------------------------------------------------------------

  This is the main visual interface with AI analysis, commit
  browser, and report viewer.

    streamlit run strapp.py

  The app will open automatically in your browser at:
    http://localhost:8501

  In the app:
    1. Select a dataset from the sidebar (pulled in Step 5)
    2. Go to the "AI Analysis" tab
    3. Click "Analyze Now" to run GPT-4 analysis
    4. View results broken down by: Root Cause, Suspect Commits,
       Error Details, Recommendations, Patterns, Summary
    5. Download the analysis as JSON if needed

----------------------------------------------------------------
  STEP 7 (ALTERNATIVE) — RUN THE COMMAND LINE SCRIPT
----------------------------------------------------------------

  If you prefer no UI, run the analysis directly from terminal:

    python app.py

  Output is printed to the terminal and saved to:
    llm_analysis.json

  Note: You need to update the data_folder path inside app.py
  main() to point to your pulled_data subfolder.

----------------------------------------------------------------
  FOLDER STRUCTURE (after full setup)
----------------------------------------------------------------

  test-engine/
  ├── strapp.py              <- Streamlit web app (main UI)
  ├── app.py                 <- Command line analysis script
  ├── git_extractor.py       <- Pulls commit data from GitHub
  ├── requirements.txt       <- All dependencies
  ├── requirements-minimal.txt
  ├── .env                   <- YOUR credentials (not in git)
  ├── .env.example           <- Template for .env
  ├── html-reports/          <- Put your HTML test reports here
  │   └── *.html
  └── pulled_data/           <- Auto-created by git_extractor.py
      └── <repo>_<timestamp>/
          ├── commits_detailed.json
          ├── repository_info.json
          └── contributors.json

----------------------------------------------------------------
  TROUBLESHOOTING
----------------------------------------------------------------

  "No data found in pulled_data/"
    -> Run git_extractor.py first (Step 5)

  "Missing credentials in .env"
    -> Make sure your .env file exists and has all 4 Azure fields:
       OPENAI_API_KEY, AZURE_OPENAI_ENDPOINT, AZURE_OPENAI_VERSION,
       AZURE_OPENAI_DEPLOYMENT

  "No test reports found in html-reports/"
    -> Create the html-reports/ folder and add .html files (Step 6)

  "ModuleNotFoundError"
    -> Make sure your virtual environment is activated and you ran:
       pip install -r requirements.txt

  "Error calling Azure OpenAI"
    -> Check your API key, endpoint URL, and deployment name.
       Ensure the deployment name in strapp.py / app.py matches
       your actual Azure deployment (default: gpt-4.1-mini)

----------------------------------------------------------------
  QUICK START SUMMARY
----------------------------------------------------------------

  git clone https://github.com/VICKYPATIL2525/test-engine.git
  cd test-engine
  python -m venv venv && venv\Scripts\activate   (Windows)
  pip install -r requirements.txt
  copy .env.example .env          <- fill in your credentials
  python git_extractor.py         <- pull repo commit data
  # add HTML reports to html-reports/ folder
  streamlit run strapp.py         <- open the app

================================================================
